*If the [notebook](https://github.com/CalebEverett/transfer-learning/blob/master/transfer-learning.ipynb) won't display online at github, you can [view it online at nbviewer](https://nbviewer.jupyter.org/github/CalebEverett/transfer-learning/blob/master/transfer-learning.ipynb).*

This notebook implements transfer learning using the [Resnet50](https://arxiv.org/abs/1512.03385) model to classify images in the [STL-10 dataset](https://cs.stanford.edu/~acoates/stl10/). 

## Approach
The [STL-10 dataset](http://cs.stanford.edu/~acoates/stl10/) consists of 5,000 training images and 8,000 test images that are 96 by 96 with three color channels selected from the [ImageNet](http://www.image-net.org/) dataset. (It also includes 100,000 unlabeled images to be used for unsupervised learning, but I won't be using those.) There are an equal number of images in each of 10 classes (airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck) for both the train (500) and test (800) sets.

The challenge is to use [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) to create a model that predicts the classes as accurately as possible on the test set as a whole. It appears that the state of the art today is 78.66% accuracy. ([This site](https://martin-thoma.com/sota/), updated as of 2017-02, had a good list of top results for many standard datasets.) But that accuracy figure is for a different protocol than the one that I'm going to execute. That protocol entailed performing unsupervised learning on the unlabeled training images and then supervised learning on the labeled data using 10 pre-defined folds, each containing 1,000 images with the overall accuracy then reported as the average of the ten separately trained models. 

I'm going to train on all 5,000 training images, and instead of using the unlabeled images, I'm going to use weights from the pretrained [Resnet50 model](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py) included with [Pytorch](http://pytorch.org/docs/master/index.html) for transfer learning. Since Resnet50 was trained on the ImageNet dataset, the results should be excellent. I checked around the internet and found other results in the mid-90%'s using a similar approach.

## Conclusion
Transfer learning based on the Resnet50 model produced excellent results in classifying images in the STL10 dataset. I was impressed with absolute results of nearly 98%. However, part of the explanation for the great results is that the STL10 images are a subset of the ImageNet images that the Resnet50 model was trained on. We are testing on images the model was trained on.

Working through the images after the model was trained was very insightful. By analyzing a few different buckets of images, I was able to develop some intuitions about how the model makes predictions, highlighting the mechanics of convolutional networks in general.

Convolutional networks, in essence, identify features and then aggregagte them through the layers of their architecture. They have proven to be exceptionally good at this task. However, the construct of a multi-classification problem and the fact that the networks don't consider important information that humans consider in classifying images leave room for improvement.

Many images actually have more than one object in them. Yet the classification problem fed into the network only identifies a single object. In many respects the vector of classification probabilities produced by the model is a much better representation of the content of an image than the single label. Take for example the first misclassified ship image. There is a ship in it, but also several birds. Only a portion of the ship is visible, but most of the birds are visible, and each has two wings, for a total of six wings in the image. In this case the convolutional network likely determined that the wings were more important than the partial ship and classified the image as an airplane (with a probability of 92% verus just 3% for the ship). The network is actually relaying back an important insight, which is that birds are similar to airplanes, but that is lost in the construct of the mutliclassification problem with only a single possible label.

Another potential problem arises when images from different classifications share features, even when those features are irrelevant to the classification problem from a human perspective. The phenomenon with the deer images being classified as cats is a perfect example. A human knows that the fence is not the subject of the images, but the convolutional network learns those features from the cat images and then uses them in its classification decisions.

Lastly, convolutional networks miss important information regarding real world size and spatial relationships. Convolutional networks aren't able to determine whether a close-up image of a cat is a different real world size than a far way image of a super tanker. Humans use this universal perspective on the world to make classifications. It is interesting to think about how convolutional architectures might be evolved to consider additional information that would improve their ability to predict classifications. I'm fascinated by the idea of [capsule](https://openreview.net/pdf?id=HJWLfGWRb) models, which as I understand it attempt to incorporate additional spatial relationship information into their arhcitectures. I'm also curious about how all of the meta information that digital cameras capture today and the three dimensional data augmented reality capable cameras capture could be utilized to improve performance.
